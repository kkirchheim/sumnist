{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1580eae6-2202-4b3e-86cb-97e91b1a98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "all_results = []\n",
    "device = \"cuda:0\"\n",
    "clip_backbone = \"ViT-L/14\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c1c4b2d-16b9-4c6b-bd8d-d07f2aeb66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from pytorch_ood.loss import DeepSVDDLoss\n",
    "from pytorch_ood.utils import OODMetrics, ToUnknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7538646-2aab-4bfa-be31-c38c91eb7dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "with np.load(\"x-test.npz\") as data:\n",
    "    x_test = torch.tensor(data['arr_0'])\n",
    "    \n",
    "with np.load(\"x-train.npz\") as data:\n",
    "    x_train = torch.tensor(data['arr_0'])\n",
    "    \n",
    "with np.load(\"y-test.npz\") as data:\n",
    "    y_test = torch.tensor(data['arr_0'])\n",
    "    y_test = torch.where(y_test.sum(dim=1) == 20, 0, -1)\n",
    "    # y_test = torch.where(y_test == 20, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff32bd0-d6b2-4050-b103-8fd05e4d7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(x_train.unsqueeze(1))\n",
    "train_loader = DataLoader(train_dataset, num_workers=5, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_test.unsqueeze(1), y_test)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, num_workers=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8e080-7a08-483a-91b1-354a3e6bea22",
   "metadata": {},
   "source": [
    "# Deep OC-SVDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8726ca3a-534d-4b51-b00e-db2726456d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(1, 16, 3, padding=1, bias=False)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.c2 = nn.Conv2d(16, 32, 3, padding=1, bias=False)\n",
    "        self.c3 = nn.Conv2d(32, 64, 3, padding=1, bias=False)\n",
    "        self.layer5 = nn.Linear(3136, 128, bias=False)\n",
    "        self.layer6 = nn.Linear(128, 2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.c1(x).relu()\n",
    "        x = self.pool(x)\n",
    "        x = self.c2(x).relu()\n",
    "        x = self.pool(x)\n",
    "        x = self.c3(x).relu()\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        x = self.layer5(x).relu()\n",
    "        x = self.layer6(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d41fc0-a6b6-42c6-a58d-5c21d8c1c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "\n",
    "opti = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "with torch.no_grad():\n",
    "    d = [model(x[0].to(device)) for x in train_loader]\n",
    "    center = torch.concat(d).mean(dim=0).cpu()\n",
    "\n",
    "criterion = DeepSVDDLoss(n_dim=2, center=center).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6f735c-b5ef-42c0-846e-d1839ffc2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    metrics = OODMetrics()\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            z = model(x.to(device))\n",
    "            # calculate (squared) distance of points to the center in output space\n",
    "            distances = criterion.distance(z)\n",
    "            # dists.append(distances)\n",
    "            # labels.append(y)\n",
    "            # print(y)\n",
    "            metrics.update(distances, y)\n",
    "    \n",
    "    m = metrics.compute()\n",
    "    # print(metrics.compute())\n",
    "    model.train()\n",
    "    return m \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbfb0bfe-8c00-4c5d-a287-1a327dfed96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, {'AUROC': 0.5085565447807312, 'AUPR-IN': 0.18896059691905975, 'AUPR-OUT': 0.8184113502502441, 'FPR95TPR': 0.9514999985694885}\n",
      "Epoch 1, {'AUROC': 0.502327561378479, 'AUPR-IN': 0.18578612804412842, 'AUPR-OUT': 0.8166112303733826, 'FPR95TPR': 0.9491999745368958}\n",
      "Epoch 2, {'AUROC': 0.49871140718460083, 'AUPR-IN': 0.18396472930908203, 'AUPR-OUT': 0.8147433400154114, 'FPR95TPR': 0.953000009059906}\n",
      "Epoch 3, {'AUROC': 0.5026273131370544, 'AUPR-IN': 0.1858845204114914, 'AUPR-OUT': 0.8150936365127563, 'FPR95TPR': 0.9584000110626221}\n",
      "Epoch 4, {'AUROC': 0.5048190355300903, 'AUPR-IN': 0.18722626566886902, 'AUPR-OUT': 0.8150094747543335, 'FPR95TPR': 0.9559000134468079}\n",
      "Epoch 5, {'AUROC': 0.5058260560035706, 'AUPR-IN': 0.18797431886196136, 'AUPR-OUT': 0.8156285285949707, 'FPR95TPR': 0.9567000269889832}\n",
      "Epoch 6, {'AUROC': 0.5055065155029297, 'AUPR-IN': 0.18745185434818268, 'AUPR-OUT': 0.8153392672538757, 'FPR95TPR': 0.9532999992370605}\n",
      "Epoch 7, {'AUROC': 0.5028736591339111, 'AUPR-IN': 0.18606741726398468, 'AUPR-OUT': 0.8153927326202393, 'FPR95TPR': 0.9556999802589417}\n",
      "Epoch 8, {'AUROC': 0.5051752924919128, 'AUPR-IN': 0.1876029074192047, 'AUPR-OUT': 0.8214966654777527, 'FPR95TPR': 0.942799985408783}\n",
      "Epoch 9, {'AUROC': 0.5058908462524414, 'AUPR-IN': 0.18714046478271484, 'AUPR-OUT': 0.8203585147857666, 'FPR95TPR': 0.9474999904632568}\n",
      "Epoch 10, {'AUROC': 0.5003798007965088, 'AUPR-IN': 0.18573567271232605, 'AUPR-OUT': 0.816851019859314, 'FPR95TPR': 0.9473000168800354}\n",
      "Epoch 11, {'AUROC': 0.4953545033931732, 'AUPR-IN': 0.18265311419963837, 'AUPR-OUT': 0.8142344355583191, 'FPR95TPR': 0.9545999765396118}\n",
      "Epoch 12, {'AUROC': 0.5003654956817627, 'AUPR-IN': 0.18551267683506012, 'AUPR-OUT': 0.8154311180114746, 'FPR95TPR': 0.9537000060081482}\n",
      "Epoch 13, {'AUROC': 0.5042688250541687, 'AUPR-IN': 0.18935561180114746, 'AUPR-OUT': 0.8176240921020508, 'FPR95TPR': 0.9473999738693237}\n",
      "Epoch 14, {'AUROC': 0.4958146810531616, 'AUPR-IN': 0.18306048214435577, 'AUPR-OUT': 0.8144645094871521, 'FPR95TPR': 0.948199987411499}\n",
      "Epoch 15, {'AUROC': 0.5038865804672241, 'AUPR-IN': 0.19026091694831848, 'AUPR-OUT': 0.818364143371582, 'FPR95TPR': 0.9473999738693237}\n",
      "Epoch 16, {'AUROC': 0.5014163851737976, 'AUPR-IN': 0.18936285376548767, 'AUPR-OUT': 0.8156360387802124, 'FPR95TPR': 0.9549000263214111}\n",
      "Epoch 17, {'AUROC': 0.5008701682090759, 'AUPR-IN': 0.18727518618106842, 'AUPR-OUT': 0.81800377368927, 'FPR95TPR': 0.9462000131607056}\n",
      "Epoch 18, {'AUROC': 0.5020588636398315, 'AUPR-IN': 0.18660582602024078, 'AUPR-OUT': 0.8152938485145569, 'FPR95TPR': 0.9490000009536743}\n",
      "Epoch 19, {'AUROC': 0.5002630949020386, 'AUPR-IN': 0.18630757927894592, 'AUPR-OUT': 0.8174678087234497, 'FPR95TPR': 0.9524999856948853}\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    for x in train_loader:\n",
    "        z = model(x[0].to(device))\n",
    "        # since this is a one-class method, we do not have to provide any class labels\n",
    "        loss = criterion(z)\n",
    "        opti.zero_grad()\n",
    "        loss.backward()\n",
    "        opti.step()\n",
    "\n",
    "    m = test()\n",
    "    print(f\"Epoch {epoch}, {m}\")\n",
    "\n",
    "m.update({\"Method\": \"Deep SVDD\", \"Backbone\": \"-\"})\n",
    "all_results.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490f8f8-1975-40e8-a8f5-3cbd6d0f3a69",
   "metadata": {},
   "source": [
    "# Using nearest neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba3795b3-e8df-4a0e-8651-4724276073b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree', n_jobs=-1).fit(x_train.view(-1, 56*56).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e27c90d1-6257-4f6a-a275-e9bf618d9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = knn.kneighbors(x_test.view(-1, 56*56).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88d51204-59f9-4b7a-aed5-28a79bfff925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.5, 'AUPR-IN': 0.5918367505073547, 'AUPR-OUT': 0.90816330909729, 'FPR95TPR': 1.0, 'Method': '1-NN', 'Backbone': '-'}\n"
     ]
    }
   ],
   "source": [
    "metrics = OODMetrics()\n",
    "metrics.update(torch.tensor(distances), y_test)\n",
    "\n",
    "m = metrics.compute()\n",
    "m.update({\"Method\": \"1-NN\", \"Backbone\": \"-\"})\n",
    "all_results.append(m)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e05f96-80aa-4104-80c4-6584183dd9f9",
   "metadata": {},
   "source": [
    "# Mahalanobis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a63417f8-eb77-417d-a429-bb59e4cfcc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.5, 'AUPR-IN': 0.5918367505073547, 'AUPR-OUT': 0.90816330909729, 'FPR95TPR': 1.0, 'Method': 'Mahalanobis', 'Backbone': '-'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "cov = EmpiricalCovariance().fit(x_train.view(-1, 56*56).numpy())\n",
    "distances = cov.mahalanobis(x_test.view(-1, 56*56).numpy())\n",
    "\n",
    "metrics = OODMetrics()\n",
    "metrics.update(torch.tensor(distances), y_test)\n",
    "m = metrics.compute()\n",
    "m.update({\"Method\": \"Mahalanobis\", \"Backbone\": \"-\"})\n",
    "all_results.append(m)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa38c2-66d8-468a-bbfe-8ca655023956",
   "metadata": {},
   "source": [
    "# Extract CLIP Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dcbbe2a-21d0-4280-81c9-09300c8572f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "# clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5edefc0e-7d1f-4dce-a31f-3ab6a8498c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 424,848\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "clip_model, preprocess = clip.load(clip_backbone)\n",
    "clip_model.cuda().eval()\n",
    "input_resolution = clip_model.visual.input_resolution\n",
    "context_length = clip_model.context_length\n",
    "vocab_size = clip_model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e4e33ab-eb2a-48ed-b250-8d8a5cff8bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-a7ad40ea4b4b>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image_input = torch.tensor(i).cuda()\n"
     ]
    }
   ],
   "source": [
    "# from pytorch_ood.utils import ToRGB\n",
    "\n",
    "# i = torch.cat([ToRGB()(x) for x in x_test.unsqueeze(1)[:100]])\n",
    "# image_input = torch.tensor(i).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99fb4afc-773a-4f28-8cf9-535be9302c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_rgb = x_test.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "x_train_rgb = x_train.unsqueeze(1).repeat(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "860b82b1-c5e5-4649-a7e1-cbbbf5b7a6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0c41045d00>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.imshow(np.moveaxis(x_test_rgb[0].numpy(), 0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0aeb68ff-044f-48ce-b9c7-6cfa33274015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ef2fd3ea53493ba01866e85188dafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "features_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(x_test_rgb.shape[0])):\n",
    "        prep = preprocess(to_pil_image(x_test_rgb[i]))\n",
    "        image_features = clip_model.encode_image(prep.cuda().unsqueeze(0)).float()\n",
    "        features_test.append(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a59518a-c8fd-44fd-b95c-fed7ef4723db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc44494597f44a6972c9c1cd564cc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_train = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(x_train_rgb.shape[0])):\n",
    "        prep = preprocess(to_pil_image(x_train_rgb[i]))\n",
    "        image_features = clip_model.encode_image(prep.cuda().unsqueeze(0)).float()\n",
    "        features_train.append(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4eaa5fa2-e906-4486-9704-0c8a3a5110aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_train = torch.cat(features_train).cpu()\n",
    "clip_test = torch.cat(features_test).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8f760-8251-44e1-af59-6df04b666338",
   "metadata": {},
   "source": [
    "# kNN-CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba43e58b-b4cf-4020-b8ee-3feb9229710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree', n_jobs=-1).fit(clip_train.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ac94002-d455-47eb-885d-32826c8e1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(clip_test.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc981b26-2685-4ec0-998e-d6078b6d1666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.6099711656570435, 'AUPR-IN': 0.24132928252220154, 'AUPR-OUT': 0.874210774898529, 'FPR95TPR': 0.8623999953269958, 'Method': '1-NN', 'Backbone': 'ViT-L/14'}\n"
     ]
    }
   ],
   "source": [
    "metrics = OODMetrics()\n",
    "metrics.update(torch.tensor(distances), y_test)\n",
    "\n",
    "m = metrics.compute()\n",
    "m.update({\"Method\": \"1-NN\", \"Backbone\": clip_backbone})\n",
    "all_results.append(m)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c5a67-0f69-4c65-8af6-4b003b1ee7cb",
   "metadata": {},
   "source": [
    "# Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0e9eba9-07c4-404e-93da-d842697e342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.5, 'AUPR-IN': 0.5918367505073547, 'AUPR-OUT': 0.8458275198936462, 'FPR95TPR': 1.0, 'Method': 'Mahalanobis', 'Backbone': 'ViT-L/14'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "cov = EmpiricalCovariance().fit(clip_train)\n",
    "distances = cov.mahalanobis(clip_test)\n",
    "\n",
    "metrics = OODMetrics()\n",
    "metrics.update(torch.tensor(distances), y_test)\n",
    "m = metrics.compute()\n",
    "m.update({\"Method\": \"Mahalanobis\", \"Backbone\": clip_backbone})\n",
    "all_results.append(m)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a722b192-8548-4918-b13e-c3b065025a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      "     &          &  AUROC &  AUPR-IN &  AUPR-OUT &  FPR95TPR \\\\\n",
      "Method & Backbone &        &          &           &           \\\\\n",
      "\\midrule\n",
      "1-NN & - &  50.00 &    59.18 &     90.82 &    100.00 \\\\\n",
      "Mahalanobis & - &  50.00 &    59.18 &     90.82 &    100.00 \\\\\n",
      "     & ViT-L/14 &  50.00 &    59.18 &     84.58 &    100.00 \\\\\n",
      "Deep SVDD & - &  50.03 &    18.63 &     81.75 &     95.25 \\\\\n",
      "1-NN & ViT-L/14 &  61.00 &    24.13 &     87.42 &     86.24 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(all_results)\n",
    "mean_scores = df.groupby([\"Method\", \"Backbone\"]).mean() * 100\n",
    "\n",
    "print(mean_scores.sort_values(\"AUROC\").to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d287c99-9bf5-45a5-af97-17b23b2dcbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clip_train, f\"clip_train_{clip_backbone.replace('/', '-')}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9860d0ed-5e82-4440-b922-135c51dcd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clip_test, f\"clip_test_{clip_backbone.replace('/', '-')}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d4578-83e2-4622-b9e6-c9cf75b9f89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
